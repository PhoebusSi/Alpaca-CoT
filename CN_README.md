[**ä¸­æ–‡**](./CN_README.md) | [**English**](./README.md)

![](./figures/å®£ä¼ 0.png)
# ç‰¹åˆ¶è‡ªå·±çš„ChatGPT: å¤šæ¥å£ç»Ÿä¸€çš„è½»é‡çº§LLM-IFTå¹³å° (**Alpaca-CoT**)
[![LICENSE](https://img.shields.io/github/license/PhoebusSi/Alpaca-CoT)](https://github.com/PhoebusSi/Alpaca-CoT/blob/main/LICENSE.txt)
[![torch](https://img.shields.io/badge/pytorch-%3E=1.13-red?logo=pytorch)](https://pytorch.org/)
[![data](https://img.shields.io/badge/huggingface-dataset-yellow?logo=data:image/png;base64,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)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)
[![model](https://img.shields.io/badge/huggingface-model-yellow?logo=data:image/png;base64,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)](https://huggingface.co/QingyiSi/Alpaca-CoT)
[![colab](https://img.shields.io/badge/Google-Colab-blue?logo=Google%20Colab)](https://colab.research.google.com/drive/1wfrKqyPkz5BGD1Gkij_cvbUeweIDdRav?usp=sharing)

è¿™æ˜¯Alpaca-CoTé¡¹ç›®çš„å­˜å‚¨åº“ï¼Œè¯¥é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªå¤šæ¥å£ç»Ÿä¸€çš„è½»é‡çº§æŒ‡ä»¤å¾®è°ƒï¼ˆIFTï¼‰å¹³å°ï¼Œè¯¥å¹³å°å…·æœ‰å¹¿æ³›çš„æŒ‡ä»¤é›†åˆ(å°¤å…¶æ˜¯CoTæ•°æ®é›†)å’Œç”¨äºå„ç§å¤§å‹è¯­è¨€æ¨¡å‹ä»¥åŠå„ç§å‚æ•°æ•ˆç‡æ–¹æ³•(å¦‚LoRAï¼ŒP-Tuning)çš„ç»Ÿä¸€æ¥å£ã€‚æˆ‘ä»¬æ­£åœ¨ä¸æ–­æ‰©å±•æˆ‘ä»¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®æ”¶é›†ï¼Œå¹¶é›†æˆæ›´å¤šçš„LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ–°å»ºäº†ä¸€ä¸ªåˆ†æ”¯[`tabular_llm`](https://github.com/PhoebusSi/Alpaca-CoT/tree/tabular_llm)æ¥æ„é€ å¯ä»¥å¤„ç†è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

<img src="./figures/image.png" width = "100" height = "100" align=right />
æ¬¢è¿æ‚¨å‘æˆ‘ä»¬æä¾›ä»»ä½•æœªæ”¶é›†çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†(æˆ–å…¶æ¥æº)ã€‚æˆ‘ä»¬å°†ç»Ÿä¸€å®ƒä»¬çš„æ ¼å¼ï¼Œç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒç¾Šé©¼æ¨¡å‹(ä»¥åŠæœªæ¥æ—©æœŸçš„å…¶ä»–LLM)ï¼Œå¼€æºæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œå¹¶è¿›è¡Œå¹¿æ³›çš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„é¡¹ç›®èƒ½å¤Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æºè¿›ç¨‹åšå‡ºå¾®è–„çš„è´¡çŒ®ï¼Œé™ä½å…¶å¯¹NLPç ”ç©¶äººå‘˜çš„å…¥é—¨é—¨æ§›ã€‚

æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©åŠ å…¥æˆ‘ä»¬çš„ç¾¤èŠ(WeChat)ï¼Œå’Œæ›´å¤šçš„åŒå¥½ç ”ç©¶è€…ä»¬äº¤æµã€‚ç›®å‰ç¾¤èŠäººæ•°è¿‡å¤šï¼Œéœ€è¦å¥½å‹é‚€è¯·æ‰èƒ½å…¥ç¾¤ï¼Œè¯·æ‰«ç åŠ æˆ‘ä¸ºå¥½å‹ï¼Œæ‹‰æ‚¨å…¥ç¾¤ã€‚

## News
- ğŸš€5.5: æ–°å»ºäº†ä¸€ä¸ªåˆ†æ”¯[`tabular_llm`](https://github.com/PhoebusSi/Alpaca-CoT/tree/tabular_llm)æ¥æ„é€ å¯ä»¥å¤„ç†å¤šç§è¡¨æ ¼æ™ºèƒ½ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚
- ğŸš€5.4: PEFTä¸­æ‰€æœ‰parameter-efficientæ–¹æ³•ï¼ˆå¦‚P-tuningï¼‰å‡è¢«é›†æˆè¿›æ¥ï¼Œå¯é€šè¿‡è¶…å‚ç®€å•è®¾ç½®ã€‚
- ğŸš€5.4: LLM `MOSS`å·²è¢«é›†æˆè¿›æ¥ã€‚
- 4.21: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `GAOKAO`, `camel`, `FLAN-Muffin`, `COIG`. 
- 4.15: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `webGPT`, `dolly`, `baize`, `hh-rlhf`, `OIG(part)`. 
- 4.12: ç°åœ¨ä½ å¯ä»¥åœ¨<a href="https://colab.research.google.com/drive/1wfrKqyPkz5BGD1Gkij_cvbUeweIDdRav?usp=sharing" >Google Colab</a>ä¸­ä½“éªŒAlpaca-CoT.
- 4.11: æ·»åŠ äº†`å¤šè½®å¯¹è¯`åŠŸèƒ½ï¼Œæ„Ÿè°¢[@paulcx](https://github.com/paulcx)ã€‚
- 4.9: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›† `firefly`, `instruct`, `Code Alpaca` [è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main).
- 4.7: æ·»åŠ äº†`å‚æ•°åˆå¹¶`ã€`æœ¬åœ°ä½¿ç”¨`ã€`æ‰¹é‡é¢„æµ‹`ã€`webæœåŠ¡`åŠŸèƒ½ï¼Œæ„Ÿè°¢[@weberrr](https://github.com/weberrr)ã€‚
- 4.4: å·²æ”¶é›†å’Œç»Ÿä¸€æ ¼å¼åŒ–æ•°æ®é›†`FastChat`, `GPTeacher`,`Guanaco`,`HC3`,`prosocial-dialog`, `belle-chat&belle-math`, `xP3` å’Œ `natural-instructions`ã€‚
- 4.3: ä¸­æ–‡çš„CoTæ•°æ®é›†`CoT_CN_data.json`å·²ä¸Šä¼ åˆ°[è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main).
- 4.1: åœ¨instinwild-CN(47k) + belle(1.5M)å¾®è°ƒå¾—åˆ°çš„Bloom7bçš„`checkpoint`å·²è¢«ä¸Šä¼ åˆ°äº†[è¿™é‡Œ](https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main).
- 4.1: `instnwild`ï¼ˆæ”¶é›†è‡ªæ¨ç‰¹ï¼Œä¸»è¦æ˜¯ç”Ÿæˆã€å¼€æ”¾å¼QAå’Œmind-storm typesï¼‰å·²ç»è¢«ç»Ÿä¸€æ ¼å¼åŒ–å¹¶æ”¶é›†.


## 0. ChatGPTèƒŒåçš„æŠ€æœ¯

**LLM**: ï¼ˆ **Large Language Models**ï¼‰æŒ‡ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒä¸”ä½“é‡è¾ƒå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œä¸€èˆ¬æ˜¯transformer-basedæ¨¡å‹ã€‚

**IFT**: ï¼ˆ **Instruction Fine-Tuning**ï¼‰æŒ‡ä»¤å¾®è°ƒï¼ŒæŒ‡ä»¤æ˜¯æŒ‡ç”¨æˆ·ä¼ å…¥çš„ç›®çš„æ˜ç¡®çš„è¾“å…¥æ–‡æœ¬ï¼ŒæŒ‡ä»¤å¾®è°ƒç”¨ä»¥è®©æ¨¡å‹å­¦ä¼šéµå¾ªç”¨æˆ·çš„æŒ‡ä»¤ã€‚

**CoT**: ï¼ˆ **Chain-of-Thought**ï¼‰æŒ‡ä»¤å½¢å¼çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼ŒåŒ…å«step-by-stepçš„æ¨ç†è¿‡ç¨‹ã€‚å¦‚ä¸‹å›¾è“è‰²éƒ¨åˆ†æ‰€ç¤ºã€‚

![cot](./figures/cot.jpg)

## 1. å®šä½

ChatGPTçš„å‡ºç°éªŒè¯äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLM)åœ¨é€šç”¨äººå·¥æ™ºèƒ½(AGI)ä¸Šçš„æ½œåŠ›ã€‚åŸºäºLLaMA[1]ç­‰Large Language Models(LLMs)çš„instruction-tuningç ”ç©¶(å¦‚ï¼ŒAlpaca[2])å¤§å¹…åº¦åŠ é€Ÿäº†å¤ç°ChatGPTçš„è¿›ç¨‹ã€‚**Alpaca-CoT**å¸Œæœ›åœ¨è¿™ä¸ªç ”ç©¶æ–¹å‘ä¸Šåšå‡ºé€‚åº¦çš„è´¡çŒ®ï¼Œä»¥æ¨è¿›LLMsçš„å¼€æºè¿›ç¨‹ã€é™ä½LLMsç ”ç©¶å’Œä½¿ç”¨æˆæœ¬ã€‚

å…·ä½“æ¥è¯´ï¼Œ**Alpaca-CoT**é¡¹ç›®æ—¨åœ¨æ¢ç©¶å¦‚ä½•æ›´å¥½åœ°é€šè¿‡instruction-tuningçš„æ–¹å¼æ¥è¯±å¯¼LLMå…·å¤‡ç±»ä¼¼ChatGPTçš„äº¤äº’å’Œinstruction-followingèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¹¿æ³›æ”¶é›†äº†ä¸åŒç±»å‹çš„instructionï¼ˆå°¤å…¶æ˜¯Chain-of-Thoughtæ•°æ®é›†ï¼‰ï¼Œå¹¶åŸºäºLLaMAç»™å‡ºäº†æ·±å…¥ç»†è‡´çš„å®è¯ç ”ç©¶ï¼Œä»¥ä¾›æœªæ¥å·¥ä½œå‚è€ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–ä¸ªå°†CoTæ‹“å±•è¿›Alpacaçš„å·¥ä½œï¼Œå› æ­¤ç®€ç§°ä¸º"**Alpaca-CoT**"ã€‚


çƒ­çƒˆæ¬¢è¿æ‚¨å‘æˆ‘ä»¬æä¾›ä»»ä½•æœªè¢«æœ¬é¡¹ç›®æ”¶é›†çš„instruction-tuningåŠå„ç±»tasksæ•°æ®é›†ï¼ˆæˆ–å…¶æ¥æºï¼‰ã€‚æˆ‘ä»¬å°†ï¼š
- å°†è¿™äº›æ•°æ®æ”¶å½•å¹¶è¿›è¡Œç»Ÿä¸€æ ¼å¼åŒ–å¤„ç†ï¼›
- ç”¨è¿™äº›æ•°æ®é›†instruct fine-tune LLaMAæ¨¡å‹ï¼ˆæœªæ¥å°†é›†æˆæ›´å¤šLLMsï¼‰ï¼Œå¹¶å¼€æºå…¶checkpointï¼›
- è¿›è¡Œå¹¿æ³›çš„å®è¯ç ”ç©¶ä»¥æ¢ç©¶æ–°æ”¶å½•çš„æ•°æ®é›†çš„ä½œç”¨ã€‚


æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„é¡¹ç›®èƒ½å¤Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æºè¿‡ç¨‹åšå‡ºé€‚åº¦çš„è´¡çŒ®ï¼Œå¹¶é™ä½NLPç ”ç©¶äººå‘˜ä¸Šæ‰‹LLMç›¸å…³ç ”ç©¶çš„é—¨æ§›ã€‚

## 2. æ¦‚è¿°

è¿‘æœŸï¼Œ[LLaMA](https://arxiv.org/abs/2302.13971v1)[1]æ˜¾ç¤ºå‡ºæƒŠäººçš„zero-shotå’Œfew-shotèƒ½åŠ›ï¼Œä»…éœ€è¾ƒå°‘çš„å‚æ•°å³å¯å’ŒGPT-3.5æ€§èƒ½ç›¸å½“ï¼ˆLLaMA-13Bæ˜¾è‘—ä¼˜äºGPT-3ï¼ˆ175Bï¼‰ï¼ŒLLaMA-65Bä¸PaLM-540MBç›¸å½“ï¼‰ï¼Œæ˜æ˜¾é™ä½äº†è®­ç»ƒã€å¾®è°ƒå’Œä½¿ç”¨competitiveå¤§å‹è¯­è¨€æ¨¡å‹çš„æˆæœ¬ã€‚æœ€è¿‘ï¼Œä¸ºäº†æé«˜LLaMAçš„instruction-followingèƒ½åŠ›ï¼Œ[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)[2]åˆ©ç”¨[self-instruct](https://arxiv.org/abs/2212.10560)[3]ç”Ÿæˆçš„52K Englishi nstruction-finetuningæ•°æ®å¯¹LLaMAè¿›è¡Œäº†å¾®è°ƒã€‚ç„¶è€Œï¼Œç›®å‰è¯¥æ–¹å‘çš„ç ”ç©¶ä»ç„¶é¢ä¸´ç€ä»¥ä¸‹ä¸‰ä¸ªæŒ‘æˆ˜ï¼š
- LLaMA-7bä¾ç„¶å¯¹è®¡ç®—èµ„æºæœ‰ç€è¾ƒé«˜çš„è¦æ±‚ï¼›
- ç”¨äºinstruction finetuningçš„å¼€æºæ•°æ®é›†è¾ƒå°‘ï¼›
- ç¼ºä¹å„instructionç±»å‹å¸¦æ¥çš„å½±å“çš„å®è¯ç ”ç©¶ï¼Œå¦‚å“åº”ä¸­æ–‡çš„èƒ½åŠ›å’ŒCoTèƒ½åŠ›ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Alpaca-CoTé¡¹ç›®ï¼Œè¯¥é¡¹ç›®ç»“åˆäº†ç›¸å…³çš„è¿‘æœŸå‰æ²¿æŠ€æœ¯ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- 1. **_ä»…éœ€è¦è¾ƒä½è®¡ç®—èµ„æºå³å¯é«˜æ•ˆå®Œæˆå¯¹LLaMAçš„å¾®è°ƒ_**ã€‚`7b`,`13b`å’Œ`30b`ç‰ˆæœ¬çš„LLaMAæ¨¡å‹å‡å¯åœ¨å•å¡80G A100ä¸Šè½»æ¾å®Œæˆè®­ç»ƒã€‚è¯¥ä¼˜åŠ¿ä¸»è¦æ¥è‡ªäº[low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf) [4], [PEFT](https://github.com/huggingface/peft)å’Œ[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)ç­‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç ä¸»è¦ä¿®æ”¹è‡ª[è¿™é‡Œ](https://github.com/tloen/alpaca-lora)ã€‚
- 2. æˆ‘ä»¬å‘å¸ƒçš„æ¨¡å‹ **_æ˜¾è‘—æå‡äº†CoT(reasoning)æ¨ç†èƒ½åŠ›_**ã€‚
- 3. æˆ‘ä»¬å‘å¸ƒçš„æ¨¡å‹ **_æ˜¾è‘—æå‡äº†å¯¹ä¸­æ–‡æŒ‡ä»¤çš„å“åº”èƒ½åŠ›_**ã€‚
- 4. ç»´æŠ¤äº†ä¸€ä¸ªä»åœ¨ä¸æ–­æ‰©å¤§è§„æ¨¡çš„ **_[intruction-finetuningçš„æ•°æ®é›†é›†åˆ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)_**ã€‚è¯¥é›†åˆåŒ…å«äº†ä¸­æ–‡ã€è‹±æ–‡å’ŒCoTçš„instructionæ•°æ®ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿç»´æŠ¤äº†ä¸€ä¸ªè®­ç»ƒè‡ªå„ç§instructionæ•°æ®é›†çš„æ¨¡å‹checkpointé›†åˆã€‚
- 5. é›†æˆäº† **_å¤šç§LLMså¹¶ç»Ÿä¸€äº†è°ƒç”¨æ¥å£_**ï¼Œå¯é€šè¿‡è¶…å‚è½»æ¾åˆ‡æ¢ã€‚ç›®å‰åŒ…å« **LLaMA, ChatGLM**[5]å’Œ **Bloom**[6]ï¼Œåç»­å°†æŒç»­åŠ å…¥æ›´å¤š,ä»¥ä¾›ç ”ç©¶è€…ä»¬è½»æ¾è°ƒç”¨å’Œå¯¹æ¯”ä¸åŒLLMsã€‚
- 6. æä¾›äº† **_è¯¦å°½é€å½»çš„å®è¯å­¦ä¹ å’Œå®šæ€§åˆ†æ_**ï¼Œè¿™é‡Œçš„findingså¯èƒ½ä¼šå¯¹ä¿ƒè¿›æœªæ¥LLMæ¢ç´¢æœ‰ä¸€å®šçš„å‚è€ƒä»·å€¼ã€‚



## 3. æ•°æ®é›†åˆ (Data Collection)  


æ”¶é›†æ•°æ®é›†çš„ç›¸å¯¹å¤§å°å¦‚ä¸‹å›¾æ‰€ç¤º:

![img](./figures/show.png)


æˆ‘ä»¬å‚è€ƒ[è¿™é‡Œ](https://github.com/yaodongC/awesome-instruction-dataset) ([@yaodongC](https://github.com/yaodongC)), å°†æ”¶é›†åˆ°çš„æ•°æ®é›†æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æ ‡æ³¨Tagsï¼š

(Lang)Lingual-Tags:
- EN: Instruction datasets in English
- CN: Instruction datasets in Chinese
- ML: [Multi-lingual] Instruction datasets in multiple languages

(Task)Task-Tags:
- MT: [Multi-task] Datasets containing multiple tasks
- TS: [Task-specific] Datasets tailored for specific tasks

(Gen)Generation-method:
- HG: [Human Generated Dataset] Datasets created by humans
- SI: [Self-Instruct] Datasets generated using self-instruct methods
- MIX: [Mixed Dataset] Dataset contains both human and machine generated data
- COL: [Collection of Dataset] Dataset made from a collection of other datasets

### æ•°æ®ç»Ÿè®¡
| æ•°æ®é›†                                                                         | æ•°ç›®      | Lang         | Task      | Gen        | ç±»å‹                                                                     | æ¥æº                                      | é“¾æ¥                                                                                       |
| :----------------------------------------------------------------------------- | :------- | :----------- | :-------- | :----------| :----------------------------------------------------------------------- | :---------------------------------------- | :---------------------------------------------------------------------------------------- |
| [Chain of Thought](https://github.com/google-research/FLAN)                    | 74771    | EN/CN        | MT        | HG         | CoTç›¸å…³ä»»åŠ¡                                                               | äººåœ¨ç°æœ‰æ•°æ®é›†ä¸Šæ ‡æ³¨CoT                    | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chain-of-Thought)     |
| [GPT4all](https://github.com/nomic-ai/gpt4all)                                 | 806199   | EN           | MT        | COL        | ä»£ç ï¼Œæ•…äº‹ï¼Œå¯¹è¯                                                           | GPT-3.5-turbo è’¸é¦                       | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GPT4all)              |
| [GPTeacher](https://github.com/teknium1/GPTeacher)                             | 29013    | EN           | MT        | SI         | é€šç”¨ï¼Œè§’è‰²æ‰®æ¼”ï¼Œå·¥å…·æŒ‡ä»¤                                                   | GPT-4 & toolformer                        | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GPTeacher)            |
| [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)       | 534610   | ML           | MT        | SI         | å¤šç§nlpä»»åŠ¡                                                               | text-davinci-003                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Guanaco)              |
| [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)                      | 37175    | EN/CN        | TS        | MIX        | å¯¹è¯è¯„ä¼°                                                                  | gpt-3.5 æˆ– äººå·¥                           | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/HC3)                  |
| [alpaca](https://github.com/tatsu-lab/stanford_alpaca)                         | 52002    | EN           | MT        | SI         | é€šç”¨æŒ‡ä»¤                                                                  | text-davinci-003                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/alpaca)               |
| [Natural Instructions](https://github.com/allenai/natural-instructions)        | 5040134  | ML           | MT        | COL        | å¤šç§nlpä»»åŠ¡                                                               | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†                     | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Natural-Instructions) |
| [belle_cn](https://huggingface.co/BelleGroup)                                  | 1079517  | CN           | TS/MT     | SI         | é€šç”¨æŒ‡ä»¤ï¼Œæ•°å­¦æ¨ç†ï¼Œå¯¹è¯                                                   | text-davunci-003                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/belle_cn)             |
| [instinwild](https://github.com/XueFuzhao/InstructionWild)                     | 52191    | EN/CN        | MT        | SI         | ç”Ÿæˆï¼Œå¼€æ”¾åŸŸé—®ç­”ï¼Œå¤´è„‘é£æš´                                                 | text-davunci-003                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/instinwild)           |
| [prosocial dialog](https://huggingface.co/datasets/allenai/prosocial-dialog)   | 165681   | EN           | TS        | MIX        | å¯¹è¯                                                                     | GPT-3æ”¹å†™é—®é¢˜ï¼Œäººå·¥å›å¤                    | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/prosocial-dialog)     |
| [finance_en](https://huggingface.co/datasets/gbharti/finance-alpaca)           | 68912    | EN           | TS        | COL        | é‡‘èé¢†åŸŸé—®ç­”                                                              | GPT3.5                                   | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/)                     |
| [xP3](https://huggingface.co/datasets/bigscience/xP3)                          | 78883588 | ML           | MT        | COL        | å¤šç§nlpä»»åŠ¡                                                               | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†                     | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/xP3)                  |
| [firefly](https://github.com/yangjianxin1/Firefly)                             | 1649398  | CN           | MT        | COL        | 23ç§nlpä»»åŠ¡                                                               | æ”¶é›†ä¸­æ–‡æ•°æ®é›†ï¼Œäººå·¥ä¹¦å†™æŒ‡ä»¤æ¨¡æ¿            | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/firefly)              |
| [instruct](https://huggingface.co/datasets/swype/instruct)                     | 888969   | EN           | MT        | COL        | GPT4Allï¼ŒAlpacaå’Œå¼€æºæ•°æ®é›†çš„å¢å¼º                                          | ä½¿ç”¨AllenAIæä¾›çš„nlpå¢å¼ºå·¥å…·               | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/instruct)             |
| [Code Alpaca](https://github.com/sahil280114/codealpaca)                       | 20022    | EN           | SI        | SI         | ä»£ç ç”Ÿæˆï¼Œç¼–è¾‘ï¼Œä¼˜åŒ–                                                       | text-davinci-003                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/CodeAlpaca)            |
| [Alpaca_GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)      | 52002    | EN/CN        | MT        | SI         | é€šç”¨æŒ‡ä»¤                                                                  | GPT-4 ç”Ÿæˆçš„Alpacaæ•°æ®                    | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/alpacaGPT4)            |
| [webGPT](https://huggingface.co/datasets/openai/webgpt_comparisons)            | 18994    | EN           | TS        | MIX        | ä¿¡æ¯æ£€ç´¢é—®ç­”                                                              | fine-tuned GPT-3 + äººå·¥è¯„ä¼°               | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/webGPT)                |
| [dolly 2.0](https://github.com/databrickslabs/dolly)                           | 15015    | EN           | TS        | HG         | å…¬å¼€ã€å°é—­å¼é—®ç­”ã€ä¿¡æ¯æŠ½å–ã€æ‘˜è¦ç”Ÿæˆã€å¼€æ”¾å¼æ„æ€ã€åˆ†ç±»ä»¥åŠåˆ›æ„å†™ä½œä¸ƒç±»ä»»åŠ¡      | äººå·¥æ ‡æ³¨                                  | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/dolly)                 |
| [baize](https://github.com/project-baize/baize-chatbot)                        | 653699   | EN           | MT        | COL        | Alpacaå’Œå¤šç§é—®ç­”ä»»åŠ¡                                                       | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†                     | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/baize)                 |
| [hh-rlhf](https://github.com/anthropics/hh-rlhf)                               | 284517   | EN           | TS        | MIX        | å¯¹è¯                                                                      | RLHF models                              | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/hh-rlhf)               |
| [OIG(part)](https://laion.ai/blog/oig-dataset/)                                | 49237    | EN           | MT        | COL        | å¤šç§nlpä»»åŠ¡                                                               | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†å’Œæ•°æ®å¢å¼º            | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/OIG)                   |
| [GAOKAO](https://github.com/OpenLMLab/GAOKAO-Bench)                            | 2785     | CN           | MT        | COL        | é«˜è€ƒä¸­çš„å¤šé€‰ï¼Œå¡«ç©ºç­‰é—®é¢˜                                                   | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†                      | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/GAOKAO)               |
| [camel](https://github.com/lightaime/camel)                                    | 760620   | EN           | MT        | SI         | ç‰©ç†ç”Ÿç‰©åŒ–å­¦ç¼–ç¨‹ï¼Œæ•°å­¦ï¼Œç¤¾ä¼šç­‰é¢†åŸŸçš„è§’è‰²æ‰®æ¼”å¯¹è¯äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†         | gpt-3.5-turbo ç”Ÿæˆ                        | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/camel)                 |
| [FLAN-Muffin](https://huggingface.co/datasets/Muennighoff/flan)                | 1764800  | EN           | MT        | COL        | 60ç§nlpä»»åŠ¡                                                              | äººå·¥æ ‡æ³¨çš„æ•°æ®é›†çš„æ”¶é›†                      | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/FLAN-Muffin)           |
| [COIG](https://huggingface.co/datasets/BAAI/COIG)                              | 298428   | CN           | MT        | COL        | è€ƒè¯•ï¼Œç¿»è¯‘ï¼Œä»·å€¼è§‚æŒ‡ä»¤æ•°æ®é›†æœé›†ï¼ŒåŸºäºçŸ¥è¯†å›¾è°±çš„åäº‹å®å¯¹è¯                    | è‡ªåŠ¨åŒ–å·¥å…·+äººå·¥éªŒè¯                         | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/COIG)                 |
| [GPT4Tools](https://github.com/StevenGrove/GPT4Tools)                          | 71446    | EN           | MT        | SI         | a collection of tool-related instructions                               | gpt-3.5-turbo                                | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/gpt4tools)             |
| [ShareChat](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)               | 1663241  | EN           | MT        | MIX        | general instruct                                                         | æ”¶é›†ShareGPT                                 | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/ShareGPT)              |
| [Auto CoT](https://github.com/amazon-science/auto-cot)                         |          | EN           |           |            |                                                                          |                                            | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Auto-CoT)              |
| [MOSS](https://github.com/OpenLMLab/MOSS)                                      | 1583595  | EN/CN        | SI        |            |                                                                          |                                            | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/MOSS)                  |
| [ultrachat](https://github.com/thunlp/UltraChat)                               | 28247446 | EN           |           |            |                                                                          |                                            | [ä¸‹è½½](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/ultrachat)             |
| [StackLLaMA](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)    | todo     | EN           |           |            |                                                                          |                                            |                                                                                                 |


è¯¥é›†åˆä»åœ¨ä¸æ–­æ›´æ–°å’Œæ‰©å¢ä¸­ã€‚å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸‹è½½å’ŒæŸ¥çœ‹æ›´å¤šæ•°æ®ç»†èŠ‚ï¼šhttps://github.com/PhoebusSi/alpaca-CoT/tree/main/data



### ä¸‹è½½
ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main)ä¸‹è½½æ‰€æœ‰æˆ‘ä»¬å·²ç»ç»Ÿä¸€æ ¼å¼åçš„formattedæ•°æ®ã€‚ç„¶åï¼Œå°†ä¸‹è½½åˆ°çš„æ–‡ä»¶å…¨éƒ¨æ”¾åˆ°[data](https://github.com/PhoebusSi/alpaca-CoT/tree/main/data) folderã€‚

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/QingyiSi/Alpaca-CoT/tree/main)ä¸‹è½½è®­ç»ƒè‡ªå„ç§ç±»å‹instructionæ•°æ®çš„æ‰€æœ‰checkpontsã€‚ç„¶åï¼Œåœ¨`generate.py`ä¸­çš„`LoRA_Weights`è®¾ç½®æˆä¸‹è½½è·¯å¾„ï¼Œå³å¯ç›´æ¥è¿è¡Œæ¨¡å‹çš„inferenceä»¥æŸ¥çœ‹æ¨¡å‹æ•ˆæœã€‚

### æ•°æ®æ ¼å¼
æˆ‘ä»¬é›†åˆä¸­çš„æ‰€æœ‰æ•°æ®å‡å·²è¢«è½¬åŒ–æˆç›¸åŒçš„æ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ ¼å¼å¦‚ä¸‹ï¼š
```
[
{"instruction": instruction string,
"input": input string, # (may be empty)
"output": output string}
]
```
æ³¨æ„ï¼Œå¯¹äºCoTæ•°æ®é›†,æˆ‘ä»¬é¦–å…ˆä½¿ç”¨FLANæä¾›çš„[template](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py)å°†å…¶ä»åŸæ•°æ®è½¬åŒ–æˆChain-of-Thoughtçš„å½¢å¼ï¼Œä¹‹åå†ç»Ÿä¸€æˆä»¥ä¸Šæ ¼å¼ã€‚æ ¼å¼ç»Ÿä¸€åŒ–çš„è„šæœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/PhoebusSi/alpaca-CoT/blob/main/data/origin_cot_data/formating.py)æ‰¾åˆ°ã€‚ 


## 4. å¤šæ¥å£ç»Ÿä¸€çš„å¼€æºå¹³å°

### ç¯å¢ƒé…ç½®
```
pip install -r requirements.txt
```
### æ¨¡å‹å¾®è°ƒ
ä¸ºäº†ä¾¿äºç ”ç©¶è€…ä»¬åœ¨LLMä¸Šåšç³»ç»Ÿçš„IFTç ”ç©¶ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸åŒç±»å‹çš„instructionæ•°æ®ï¼Œé›†æˆäº†å¤šç§LLMï¼Œå¹¶ç»Ÿä¸€äº†æ¥å£ï¼Œå¯ä»¥è½»æ¾å®šåˆ¶åŒ–æƒ³è¦çš„æ­é…ï¼š
- `--model_type`: è®¾ç½®æƒ³è¦ç ”ç©¶çš„LLMï¼Œç›®å‰å·²æ”¯æŒ[llama, chatglmå’Œbloom]ï¼Œå…¶ä¸­åä¸¤è€…çš„ä¸­æ–‡èƒ½åŠ›è¾ƒå¼ºï¼Œåç»­å°†ä¼šé›†æˆæ›´å¤šçš„LLMsã€‚
- `--data`: è®¾ç½®ç”¨ä»¥IFTçš„æ•°æ®ç±»å‹ï¼Œä»¥çµæ´»ç‰¹åˆ¶æƒ³è¦çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¦‚è¿½æ±‚è¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›å¯è®¾ç½®alpaca-cotï¼Œè¾ƒå¼ºçš„ä¸­æ–‡èƒ½åŠ›å¯è®¾ç½®belle1.5mï¼Œè¾ƒå¼ºçš„codingå’Œæ•…äº‹åˆ›ä½œèƒ½åŠ›å¯è®¾ç½®gpt4allï¼Œé‡‘èç›¸å…³çš„å“åº”èƒ½åŠ›å¯è®¾ç½®financeã€‚
- `--model_name_or_path`: ä¸`--model_type`ç›¸å¯¹åº”ï¼Œç”¨æ¥åŠ è½½ç›®æ ‡LLMçš„ä¸åŒå‹å·æƒé‡ã€‚å¦‚ï¼Œè¦åŠ è½½llamaçš„13bçš„æ¨¡å‹æƒé‡æ—¶å¯è®¾ç½®decapoda-research/llama-13b-hfã€‚ 

**å•å¡**
- for LLaMA
```
python3 uniform_finetune.py --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \
    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
    
```

- for ChatGLM
Note: for multiple datasets, you can use `--data` like `--data ./data/alpaca.json ./data/finance.json <path2yourdata_1>`
```
python3 uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \
    --learning_rate 2e-5 --epochs 1
```
Note that `load_in_8bit` is not yet suitable for ChatGLM, so batch_size must be much smaller than others. 

- for BLOOM
```
python3 uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
```

Note that you can also pass the local path (where the LLM weights saved) to `--model_name_or_path`. And the data type `--data` can be freely set according to your interests.

**å¤šå¡**
- for LLaMA
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy uniform_finetune.py \
    --model_type llama --model_name_or_path decapoda-research/llama-7b-hf \
    --data alpaca-belle-cot --lora_target_modules q_proj v_proj \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1 
```
- for ChatGLM
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \
    uniform_finetune.py   --model_type chatglm --model_name_or_path THUDM/chatglm-6b \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --lora_r 32 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 2 \
    --learning_rate 2e-5 --epochs 1
```
Note that `load_in_8bit` is not yet suitable for ChatGLM, so batch_size must be much smaller than others. 

- for BLOOM
```
python3 -m torch.distributed.launch --nproc_per_node 4  \
    --nnodes=1 --node_rank=0 --master_addr=xxx --master_port=yyy \
    uniform_finetune.py   --model_type bloom --model_name_or_path bigscience/bloomz-7b1-mt \
    --data alpaca-belle-cot --lora_target_modules query_key_value \
    --per_gpu_train_batch_size 4 --learning_rate 3e-4 --epochs 1  
```
### Inference
``` 
python3 generate.py  --data alpaca-belle-cot --model_type llama

python3 generate.py  --data alpaca-belle-cot --model_type chatglm

python3 generate.py  --data alpaca-belle-cot --model_type bloom
```

æ³¨æ„ï¼Œ`saved-xxx7b` æ–‡ä»¶å¤¹æ˜¯ä¿å­˜LoRA weightsçš„è·¯å¾„ï¼Œè€ŒLLaMAçš„weightsåˆ™ä¼šåœ¨è„šæœ¬æ‰§è¡ŒæœŸé—´è‡ªåŠ¨ä»Hugging Faceä¸Šä¸‹è½½ã€‚

### ç”Ÿæˆç›¸å…³è¶…å‚è®¾ç½®
```
top_p=0.9, 
        #é€‚åº¦è°ƒå¤§æ ¸é‡‡æ ·çš„æ¦‚ç‡é˜ˆå€¼ï¼Œæ‰©å¤§å€™é€‰å­é›†ï¼Œå¢åŠ ç”Ÿæˆå¤šæ ·æ€§ã€‚
        
temperature=1.0, 
        #ä¹‹å‰çš„æ¸©åº¦å‚æ•°è¿‡ä½ä¼šå¯¼è‡´ç”Ÿæˆè¯çš„æ¦‚ç‡åˆ†å¸ƒæåŒ–ä¸¥é‡ï¼Œå¯¼è‡´ç”Ÿæˆç­–ç•¥é€€åŒ–æˆgreedy decodingã€‚
        
do_sample=True, 
        #do_sampleå‚æ•°é»˜è®¤å…³é—­ï¼Œä¸å¼€å¯æ—¶ç”Ÿæˆä»ä¿æŒbeam-searchè§£ç ç­–ç•¥ï¼Œå¼€å¯åä¸ºbeam-search multinomial samplingè§£ç ç­–ç•¥ã€‚
        
no_repeat_ngram_size=6, 
        #é€šè¿‡é…ç½®ä¸‹ä¸€ä¸ªè¯é‡å¤å‡ºç°n-gramçš„æ¦‚ç‡ä¸º0ï¼Œæ¥ä¿è¯æ²¡æœ‰n-gramå‡ºç°ä¸¤æ¬¡ï¼Œè®¾ç½®è¿‡å°ä¼šæŠ‘åˆ¶åˆç†çš„é‡å¤ï¼Œå½±å“ç”Ÿæˆçš„æµç•…æ€§ï¼Œè¿‡å¤§ä¼šå¤±å»ä½œç”¨ã€‚
        
repetition_penalty=1.8, 
        #å¯¹äºä¹‹å‰å‡ºç°è¿‡çš„è¯è¯­ï¼Œåœ¨åç»­é¢„æµ‹çš„è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å¼•å…¥æƒ©ç½šå› å­repetition_penaltyé™ä½å…¶å‡ºç°çš„æ¦‚ç‡ã€‚
```

### å‚æ•°åˆå¹¶
```
python3 merge.py --model_type llama --size 7b --lora_dir xxx --merged_dir yyy
```
### æœ¬åœ°ä½¿ç”¨
```
python3 server.py --model_type chatglm --lora_dir xxx
```
### æ‰¹é‡é¢„æµ‹
```
python3 predict.py --model_type chatglm --data for_dict_data --lora_dir xxx --result_dir yyy
```

### webæœåŠ¡

```
python3 web.py --model_type chatglm --lora_dir xxx
```


## 5. Quantitative Analysis
æ³¨æ„ï¼šä¸‹å›¾æ˜¯æˆªæ­¢åˆ°3.26æ—¥æ”¶é›†åˆ°çš„æ•°æ®é›†çš„ç»Ÿè®¡æƒ…å†µï¼Œä»…ä½œä¸ºmotivationå±•ç¤ºã€‚ç›®å‰å·²æ”¶é›†äº†æ›´å¤šæ•°æ®é›†ï¼Œå¦‚é‡‘èç›¸å…³çš„æŒ‡ä»¤æ•°æ®é›†ã€‚
![data collection statistics](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/piechart.png)
å½“å‰çš„instruction-finetuningæ•°æ®é›†åˆä¸»è¦åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š
- `alpaca_data_cleaned.json`: about 52K English instruction-following training samples.
- `CoT_data.json`: 9 CoT datasets involving about 75k samples. ï¼ˆç›¸å…³çš„CoTæ•°æ®é›†ç”±FLAN[7]å‘å¸ƒï¼‰
- `belle_data_cn.json`:  about 0.5M Chinese |instruction-following training samples. ï¼ˆç›¸å…³çš„ä¸­æ–‡instructionæ•°æ®ç”±BELLE[8]å‘å¸ƒï¼‰

### å…³äºCoTå’ŒChinese Instructionsçš„æ¶ˆè
"w/o CoT" and "w/o CN" åˆ†åˆ«è¡¨ç¤ºç”¨åœ¨instruction-finetuningæœŸé—´ä¸é‡‡ç”¨CoTæ•°æ®å’ŒChinese instructionsã€‚

éœ€è¦æ¨ç†èƒ½åŠ›çš„é—®é¢˜ä¸Šçš„è¡¨ç°
![f3](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾3.png)
 
éœ€è¦éµå¾ªä¸­æ–‡æŒ‡ä»¤çš„é—®é¢˜ä¸Šçš„è¡¨ç°
![f4](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾4.png)
 
åœ¨è¾ƒå¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°
![f5](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾5.png)

          

**In summary, the models finetuned from our complete dataset (English, Chinese, and CoT instruction data) can significantly improve model reasoning and Chinese instruction following abilities.**


### æ›´å¤šèƒ½åŠ›å±•ç¤º

![ablation-cot](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾6.png)


![ablation-cot](https://github.com/PhoebusSi/alpaca-CoT/blob/main/figures/å›¾8.png)





## å‚è€ƒæ–‡çŒ®

[1]: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

[2]: [Stanford Alpaca: An Instruction-following LLaMA model](https://github.com/tatsu-lab/stanford_alpaca)

[3]: [Self-Instruct: Aligning Language Model with Self Generated Instructions](https://arxiv.org/abs/2212.10560)

[4]: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)

[5]: [ChatGLM: An Open Bilingual Dialogue Language Model](https://github.com/THUDM/ChatGLM-6B)

[6]: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)

[7]: [FLAN: Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

[8]: [BELLE: Bloom-Enhanced Large Language model Engine](https://github.com/LianjiaTech/BELLE)

[9]: [GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo](https://github.com/nomic-ai/gpt4all)



## Citation
Please cite the repo if you use the data collection, code, and experimental findings in this repo. 
```
@misc{alpaca-cot,
  author = {Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, Zheng Lin },
  school = {Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China},
  title = {Alpaca-CoT: An Instruction Fine-Tuning Platform with Instruction Data Collection and Unified Large Lnguage Models Interface},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/PhoebusSi/alpaca-CoT}},
}
```
